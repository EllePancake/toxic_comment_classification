{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from torch.nn import Sigmoid\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up \n",
    "\n",
    "The goal of this model is to find the optimal learning rate, which will use the same architecture as **Model 3** (DistilBERT for multi-label classification with focal loss) but with a modified learning rate. \n",
    "\n",
    "Given that training for 5 epochs takes around 8 hours, it's important to optimize the learning rate efficiently without conducting full-length training cycles. To achieve this, we will evaluate 6 different learning rates by training the model for a reduced number of epochs (3 epochs per learning rate). The goal is to identify which learning rate produces the best performance in terms of average loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Four"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLabelDistilBERT(\n",
      "  (base_model): DistilBertForSequenceClassification(\n",
      "    (distilbert): DistilBertModel(\n",
      "      (embeddings): Embeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (transformer): Transformer(\n",
      "        (layer): ModuleList(\n",
      "          (0-5): 6 x TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    problem_type=\"multi_label_classification\", \n",
    "    num_labels=6)\n",
    "\n",
    "class MultiLabelDistilBERT(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(MultiLabelDistilBERT, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs\n",
    "\n",
    "model = MultiLabelDistilBERT(model)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelDistilBERT(\n",
       "  (base_model): DistilBertForSequenceClassification(\n",
       "    (distilbert): DistilBertModel(\n",
       "      (embeddings): Embeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layer): ModuleList(\n",
       "          (0-5): 6 x TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentsDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text = dataframe['cleaned_comment_text']  \n",
    "        self.labels = dataframe[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment = str(self.comment_text.iloc[index])  \n",
    "        inputs = self.tokenizer(\n",
    "            comment,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(self.labels[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ToxicCommentsDataset(train_df, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        return torch.mean(F_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing learning rate: 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epoch 1/3: 100%|██████████| 9974/9974 [2:28:21<00:00,  1.12it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 1e-06, Epoch 1/3, Average Loss: 1.2834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/3: 100%|██████████| 9974/9974 [2:27:00<00:00,  1.13it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 1e-06, Epoch 2/3, Average Loss: 0.4868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/3: 100%|██████████| 9974/9974 [2:29:25<00:00,  1.11it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 1e-06, Epoch 3/3, Average Loss: 0.4066\n",
      "\n",
      "Testing learning rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epoch 1/3: 100%|██████████| 9974/9974 [2:29:56<00:00,  1.11it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 1e-05, Epoch 1/3, Average Loss: 0.5185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/3: 100%|██████████| 9974/9974 [2:30:05<00:00,  1.11it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 1e-05, Epoch 2/3, Average Loss: 0.3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/3: 100%|██████████| 9974/9974 [2:29:52<00:00,  1.11it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 1e-05, Epoch 3/3, Average Loss: 0.2621\n",
      "\n",
      "Testing learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epoch 1/3: 100%|██████████| 9974/9974 [2:29:02<00:00,  1.12it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.0001, Epoch 1/3, Average Loss: 0.7394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/3: 100%|██████████| 9974/9974 [2:29:19<00:00,  1.11it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.0001, Epoch 2/3, Average Loss: 0.9385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/3: 100%|██████████| 9974/9974 [2:35:11<00:00,  1.07it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.0001, Epoch 3/3, Average Loss: 1.0982\n",
      "\n",
      "Testing learning rate: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epoch 1/3: 100%|██████████| 9974/9974 [2:39:00<00:00,  1.05it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.0005, Epoch 1/3, Average Loss: 1.1334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/3: 100%|██████████| 9974/9974 [2:42:09<00:00,  1.03it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.0005, Epoch 2/3, Average Loss: 1.1205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/3: 100%|██████████| 9974/9974 [2:38:34<00:00,  1.05it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.0005, Epoch 3/3, Average Loss: 1.1174\n",
      "\n",
      "Testing learning rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epoch 1/3: 100%|██████████| 9974/9974 [2:33:01<00:00,  1.09it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.001, Epoch 1/3, Average Loss: 1.1424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/3: 100%|██████████| 9974/9974 [2:33:18<00:00,  1.08it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.001, Epoch 2/3, Average Loss: 1.1312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/3: 100%|██████████| 9974/9974 [2:33:30<00:00,  1.08it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.001, Epoch 3/3, Average Loss: 1.1239\n",
      "\n",
      "Testing learning rate: 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Training Epoch 1/3: 100%|██████████| 9974/9974 [2:35:04<00:00,  1.07it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.005, Epoch 1/3, Average Loss: 1.1903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/3: 100%|██████████| 9974/9974 [2:30:17<00:00,  1.11it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.005, Epoch 2/3, Average Loss: 1.1140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/3: 100%|██████████| 9974/9974 [2:35:55<00:00,  1.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.005, Epoch 3/3, Average Loss: 1.0886\n",
      "\n",
      "Learning Rate Testing Complete!\n",
      "    Learning Rate Epoch  Avg Loss\n",
      "0        0.000001     1  1.283355\n",
      "1        0.000001     2  0.486803\n",
      "2        0.000001     3  0.406599\n",
      "3        0.000010     1  0.518489\n",
      "4        0.000010     2  0.319968\n",
      "5        0.000010     3  0.262056\n",
      "6        0.000100     1  0.739433\n",
      "7        0.000100     2  0.938457\n",
      "8        0.000100     3  1.098237\n",
      "9        0.000500     1  1.133377\n",
      "10       0.000500     2  1.120493\n",
      "11       0.000500     3  1.117374\n",
      "12       0.001000     1  1.142442\n",
      "13       0.001000     2  1.131222\n",
      "14       0.001000     3  1.123862\n",
      "15       0.005000     1  1.190259\n",
      "16       0.005000     2  1.113955\n",
      "17       0.005000     3  1.088575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = FocalLoss()\n",
    "\n",
    "learning_rates = [1e-6, 1e-5, 1e-4, 5e-4, 1e-3, 5e-3]\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Learning Rate', 'Epoch', 'Avg Loss'])\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTesting learning rate: {lr}\")\n",
    "    \n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased',\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        num_labels=6\n",
    "    )\n",
    "    model = MultiLabelDistilBERT(model)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Learning Rate: {lr}, Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        new_row = pd.DataFrame([{\n",
    "        'Learning Rate': lr,\n",
    "        'Epoch': epoch+1,\n",
    "        'Avg Loss': avg_loss\n",
    "        }])\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "results_df.to_csv('learning_rate_results.csv', index=False)\n",
    "\n",
    "print(\"\\nLearning Rate Testing Complete!\")\n",
    "print(results_df)\n",
    "#2749m 11.0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for the model with `lr=5e-5`:\n",
    "- **Epoch 1**: Average Loss = 0.0129\n",
    "- **Epoch 2**: Average Loss = 0.0099\n",
    "- **Epoch 3**: Average Loss = 0.0082\n",
    "- **Epoch 4**: Average Loss = 0.0070\n",
    "- **Epoch 5**: Average Loss = 0.0059\n",
    "\n",
    "### Results from your learning rate testing:\n",
    "\n",
    "| Learning Rate | Epoch 1 Avg Loss | Epoch 2 Avg Loss | Epoch 3 Avg Loss |\n",
    "|---------------|------------------|------------------|------------------|\n",
    "| `1e-6`        | 1.283355         | 0.486083         | 0.406599         |\n",
    "| `1e-5`        | 0.518489         | 0.319068         | 0.262956         |\n",
    "| `1e-4`        | 1.098237         | 0.938457         | 0.739433         |\n",
    "| `5e-4`        | 1.133377         | 1.120493         | 1.117374         |\n",
    "| `1e-3`        | 1.144442         | 1.142322         | 1.123862         |\n",
    "| `5e-3`        | 1.190259         | 1.113955         | 1.088575         |\n",
    "\n",
    "### Comparison:\n",
    "\n",
    "- The model with `lr=5e-5` shows **significantly lower loss** compared to the learning rate tests, especially when compared to `1e-6` and `1e-5`. \n",
    "- The lowest loss achieved during testing was for **`1e-5`** with a loss of **0.262956** in epoch 3. However, with `5e-5`, your model achieved an even lower loss of **0.0059** in epoch 5, which indicates a much better performance.\n",
    "\n",
    "The model trained with **`lr=5e-5`** provided significantly better results (lower loss) than any of the learning rates you tested earlier. Given the excellent results across all epochs, it seems that this learning rate is highly effective for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of Learning Rate Findings:\n",
    "\n",
    "In the previous experiments, I conducted a search for the optimal learning rate by testing multiple values: `[1e-6, 1e-5, 1e-4, 5e-4, 1e-3, 5e-3]`. These learning rates were tested across 3 epochs each, and the average loss for each learning rate and epoch was tracked to determine the best-performing rate. The goal was to identify a learning rate that minimizes the loss and improves model performance.\n",
    "\n",
    "However, when comparing these results with **Model 3**, which was trained as a benchmark with a learning rate of **5e-5**, the findings showed that the model trained with **5e-5** consistently achieved significantly lower losses across all epochs compared to the tested learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations for Improvement\n",
    "\n",
    "   - **Model Improvements:** \n",
    "     - Using more advanced models \n",
    "     - Hyperparameter tuning\n",
    "     - Implementing different preprocessing techniques\n",
    "     - Using ensemble methods\n",
    "   - **Data Augmentation:** data augmentation techniques that could help with class imbalance or improve the model’s generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
